<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project #4A</title>
    <style>
        body { font-family: Cambria, sans-serif; margin: 75px 200px 75px 200px;; line-height: 1.5;}
        header, section { margin-bottom: 45px; }
        header, h1, h2, h3, h4 { color: #5A5FAA; }
        p {font-size: 1.1em;  margin-top: 0px; line-height: 1.35; }
        h2 { margin-bottom: 0px; line-height: 1.0; }
        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 10px auto; 
        }
        .code {
            font-family: 'Courier New', Courier, monospace; 
            background-color: #f4f4f4;
            color: #410c19; 
            padding: 2px 4px;
            border-radius: 4px;
            font-size: 0.9em;
        }

        @media print {
            body {
                font-family: Cambria, serif; /* Optional: Change font for print */
                margin: 10px; /* Reduced margin for print */
                font-size: 12pt; /* Standard print font size */
                line-height: 1.3; /* Adjust line height for better readability in print */
            }

            header, h1, h2, h3 {
                font-size: 14pt; 
                margin: 10px;
                padding: 0; 
            }

            p {
                font-size: 12pt; 
                margin: 10px;
                padding: 0; 
            }

            header {
                display: block; /* Ensures header is visible with appropriate sizing */
            }

            .no-print {
                display: none; /* Elements with class 'no-print' will not be shown */
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <header>
        <h1>CS180 Project #5: Fun With Diffusion Models</h1>
    </header>
    <hr>

    <h1>Part A: The Power of Diffusion Models</h1>


    <p>
        In the following section, a pretrained DeepFloyd denoisers with \(T=1000\) is used to produce high quality images as the description. 
    </p>

    <section>
        <h2>Part 0: Setup</h2>

        <p>
            <p>
                For the 3 text prompts, the caption and the output of the model for <span class="code">num_inference_steps=5, 100</span> with seed=0 are displayed below:
            </p>

           <div class="image-item"></div>
            <img src=".\out\5a_0_1.png" alt="p1_visual", style="max-height: 400px;">
            <img src=".\out\5a_0_2.png" alt="p1_visual", style="max-height: 400px;">
            </div>

            <p>
                It can be seen that the model generate images resembling the text prompts, and as the <span class="code">num_inference_steps</span>
                 increases, the images have higher quality and more details. And after passed into <span class="code">stage_2</span>, the quality improves.
            </p>
        </p>
    </section>
    <hr>

    <section>
        <h2>Part 1: Sampling Loops: 1.1 Implementing the Forward Process</h2>

        <p>
            <p>
            The forward process takes a clean image and adds noise to it. It is defined by: $$q(x_t | x_0) = N(x_t ; \sqrt{\bar\alpha} x_0, (1 -
            \bar\alpha_t)\mathbf{I})$$ which is equivalent to computing
            $$ x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon
            \quad \text{where}~ \epsilon \sim N(0, 1) $$

            That is, given a clean image  \(x_0\), we get a noisy image \(x_t\) at
            timestep \(t\) by sampling from a Gaussian with mean \(\sqrt{\bar\alpha_t}x_0 \) and variance \( (1 - \bar\alpha_t) \).
            </p>
            <p>
            The test images at noise level [250, 500, 750] are shown below:
            </p>

           <div class="image-item"></div>
            <img src=".\out\5a_1_1.png" alt="p1_visual", style="max-height: 300px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.2 Classical Denoising</h2>

        <p>
            <p>
                Take noisy images for timesteps [250, 500, 750], and use
                <span class="code">torchvision.transforms.functional.gaussian_blur</span> to remove the noise.

            </p>

            <p>
                The test images using <span class="code">kernel_size=(5, 5), sigma=(1, 1)</span> are shown below:
            </p>

           <div class="image-item"></div>
            <img src=".\out\5a_1_2.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.3 One-Step Denoising</h2>

        <p>
            <p>
                In this section, a pretrained diffusion model is used to recover Gaussian noise from the image,
                Then, we can remove this noise to recover the original image.  
                Because this diffusion model was trained with text conditioning, a text prompt embedding for the prompt "a high quality photo" is applied.
            </p>

            <p>
                The original image, the noisy image, and the estimate of the original image are shown below:
            </p>

           <div class="image-item"></div>
            <img src=".\out\5a_1_3.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.4 Iterative Denoising</h2>

        <p>
            <p>
                In this section, the diffusion model is used to denoise iteratively for \(T=1000\). 
                The process is sped up by skipping steps by creating a new list of timesteps. 
                On the ith denoising step, apply the following formula for update:
                $$ x_{t'} = \frac{\sqrt{\bar\alpha_{t'}}\beta_t}{1 - \bar\alpha_t} x_0 +
                    \frac{\sqrt{\alpha_t}(1 - \bar\alpha_{t'})}{1 - \bar\alpha_t} x_t +
                    v_\sigma$$

                    <p class="text">
                        where:
                      </p>
                      <ul>
                        <li>\(x_t\) is the image at timestep \(t\)</li>
                        <li>\(x_{t'}\) is the noisy image at timestep \(t'\) where \(t' < t\) </li>
                        <li>\(\bar\alpha_t\) is defined by <code>alphas_cumprod</code></li>
                        <li>\(\alpha_t = \frac{\bar\alpha_t}{\bar\alpha_{t'}}\)</li>
                        <li>\(\beta_t = 1 - \alpha_t\)</li>
                        <li>\(x_0\) is the current estimate of the clean image</li>
                      </ul>
              
                      <p class="text"></p>
                
            </p>

            <p>
                The noisy image every 5th loop of denoising, the final predicted clean image using iterative denoising with previous denoised images are shown below:
            </p>

           <div class="image-item"></div>
            <img src=".\out\5a_1_4_1.png" alt="p1_visual", style="max-height: 200px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5a_1_4_2.png" alt="p1_visual", style="max-height: 200px;">
            </div>
        </p>
    </section>
    <hr>

    
    <section>
        <h2>1.5 Diffusion Model Sampling</h2>

        <p>
            <p>
                By setting <span class="code">i_start = 0</span> and passing in random noise, images can be generated from scratch.
            </p>

            <p>
                5 sampled images are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_5.png" alt="p1_visual", style="max-height: 200px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.6 Classifier-Free Guidance (CFG)</h2>
        <p>
            <p>
                By using a technique called Classifier-Free Guidance, the image quality can be greatly improved.
                In CFG, both a conditional and an unconditional noise estimate are computed, denoted as 
                \( \epsilon_c \) and \( \epsilon_u \).
                Then, the new noise estimate be: $$\epsilon = \epsilon_u + \gamma
                (\epsilon_c - \epsilon_u) $$
                where \( \gamma \) controls the strength of CFG. 
                When \( \gamma > 1 \), the image quality can be much higher.
            </p>

            <p>
                5 images of "a high quality photo" with a CFG scale of \( \gamma > 7 \) are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_6.png" alt="p1_visual", style="max-height: 200px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.7 Image-to-image Translation </h2>

        <p>
            <p>
                The SDEdit algorithm is used such that the original test image is noised a little, and be forced back onto the image manifold without any conditioning
                to create an image that is similar to the test image.
            </p>

            <p>
                Edits of the test images, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] with text prompt "a high quality photo" are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_7.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.7.1 Editing Hand-Drawn and Web Images </h2>

        <p>
            <p>
                This procedure works particularly well if starting with a nonrealistic image and project it onto the natural image manifold.
                For noise levels [1, 3, 5, 7, 10, 20], 1 image from the web and 2 hand drawn images are shown below.
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_7_1.png" alt="p1_visual", style="max-height: 600px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5a_1_7_1_2.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.7.2 Inpainting </h2>

        <p>
            <p>
                The section implements inpainintg, that is, given an image \( x_{orig} > 1 \), and a binary mask \( m \), we can create a new image that has the same content where 
                \( m \) is 0, but new content wherever  \( m \) is 1:

                $$ x_t \leftarrow \textbf{m} x_t + (1 - \textbf{m})
                \text{forward}(x_{orig}, t) $$
            </p>

            <p>
                The test images inpainted are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_7_2.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.7.3 Text-Conditional Image-to-image Translation </h2>

        <p>
            <p>
                This section applies SDEdit, but guide the projection with a text prompt.
            </p>

            <p>
                The test images edited at noise levels [1, 3, 5, 7, 10, 20] are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_7_3.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.8 Visual Anagrams </h2>

        <p>
            <p>
                This section implements Visual Anagrams, and create optical illusions with diffusion models.
                In this part, it will create an image that looks like one prompt, but when flipped upside down will reveal another prompt.
                The image \( x_t \) at step \( t \) is denoised normally with the first prompt, to obtain noise estimate \( \epsilon_1 \),
                at the same time flip \( x_t \) and denoise with the second prompt to obtain noise estimate \( \epsilon_2 \) and flip it back.
                Then perform a reverse/denoising diffusion step with the averaged noise estimate.
                The full algorithm is:
                $$ \epsilon_1 = \text{UNet}(x_t, t, p_1) $$
                $$ \epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2)) $$
                $$ \epsilon = (\epsilon_1 + \epsilon_2) / 2 $$
            </p>

            <p>
                3 visual anagrams are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_8.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.9 Hybrid Images</h2>

        <p>
            <p>
                This section implements Factorized Diffusion and create hybrid images. 
                First create a composite noise estimate, by estimating the noise with two different text prompts, 
                and then combining low frequencies from one noise estimate with high frequencies of the other. By using a gaussian blur of kernel size 33 and sigma 2, the algorithm is:
                $$ \epsilon_1 = \text{UNet}(x_t, t, p_1) $$
                $$ \epsilon_2 = \text{UNet}(x_t, t, p_2) $$
                $$ \epsilon = f_\text{lowpass}(\epsilon_1) + f_\text{highpass}(\epsilon_2)$$
            </p>

            <p>
                3 hybrid images are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5a_1_10.png" alt="p1_visual", style="max-height: 200px;">
            </div>
        </p>
    </section>
    <hr>

    <h1>Part B: Diffusion Models from Scratch</h1>

    <section>
        <h2>Part 1: Training a Single-Step Denoising UNet</h2>
        <p>
        <p>
            This section builds a simple one-step denoiser. 
            Given a noisy image
            \( z \), we
            aim to train a denoiser \( D_\theta \) such that it maps \( z \) to a clean
            image \( x \). To do so, we can optimize over an L2 loss:
            $$L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2 $$

        </p>
        </p>
    </section>

    <section>
        <h2>1.1 Implementing the UNet</h2>

        <p>
            <p>
                The implementation follows the diagrams below: 
            </p>

            <div class="image-item"></div>
            <img src=".\out\unconditional_arch.png" alt="p1_visual", style="max-height: 400px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\atomic_ops_new.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.2 Using the UNet to Train a Denoiser </h2>

        <p>
            <p>
                To train our denoiser, we need to generate training data pairs of (\( z \), \( x \)), where each 
                is a clean MNIST digit. For each training batch, we can generate \( z \) from \( x \) using the the following noising process:  
                $$
                z = x + \sigma \epsilon,\quad \text{where }\epsilon \sim N(0, I)
                $$
            </p>

            <p>
                The different noising processes over \(\sigma = [0.0, 0.2, 0.4,
                0.5, 0.6, 0.8, 1.0]\), assuming normalized \(x \in [0, 1]\), are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5b_1_1.png" alt="p1_visual", style="max-height: 600px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.2.1 Training </h2>

        <p>
            <p>
                The section trains a denoiser to denoise noisy image \( z \) with \(\sigma = 0.5\) applied to a clean image \( x \),
                with dataset <span class="code">torchvision.datasets.MNIST</span>, with batch size 256, for 5 epochs. 
                The UNet architecture is defined with hidden dimension D = 128. The Adam optimizer is used with learning rate of 1e-4.
            </p>

            <p>
                The denoised results on the test set after epoch=1 and epoch=5, along with the Training Loss Curve are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5b_1_2_1.png" alt="p1_visual", style="max-height: 400px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_1_2_5.png" alt="p1_visual", style="max-height: 400px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_1_2_loss.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>1.2.2 Out-of-Distribution Testing </h2>

        <p>

            <p>
                The denoiser results on test set digits with varying levels of noise \(\sigma = [0.0, 0.2, 0.4,
                0.5, 0.6, 0.8, 1.0]\) are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5b_1_2_result.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>Part 2: Training a Diffusion Model</h2>
        <p>
            <p>
                This section implements DDPM. The UNet is changed to predict the added noise \( \epsilon \).
                The loss function is updated as:

                $$L = \mathbb{E}_{\epsilon,x_0,t} \|\epsilon_{\theta}(x_t, t) -
                \epsilon\|^2
                $$

                <ul>
                    <li>Create a list \(\beta\) of length \(T\) such that \(\beta_0 = 0.0001\) and \(\beta_T = 0.02\) and all other elements \(\beta_t\) for \(t \in \{1, \cdots, T-1\}\) are evenly spaced between the two.</li>
                    <li>\(\alpha_t = 1 - \beta_t\)</li>
                    <li>\(\bar\alpha_t = \prod_{s=1}^t \alpha_s\) is a cumulative product of \(\alpha_s\) for \(s \in \{1, \cdots, t\}\)</li>
                  </ul>
            </p>
        </p>
    </section>

    <section>
        <h2>2.1 Adding Time Conditioning to UNet</h2>

        <p>

            <p>
                The conditioned UNet architecture is shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\conditional_arch.png" alt="p1_visual", style="max-height: 600px;">
            </div>
            
            <div class="image-item"></div>
            <img src=".\out\fc_long.png" alt="p1_visual", style="max-height: 200px;">
            </div>
        </p>
    </section>
    <hr>

    <section>
        <h2>2.2 Training the UNet & 2.3 Sampling from the UNet</h2>

        <p>

            <p>
                To train the time-conditioned model, pick a random image from the training set, a random \(t\)
                , and train the denoiser to predict the noise in \(x_t\).
                 Repeat this for different images and different \( t \) values until the model converges.
            </p>


            <div class="image-item"></div>
            <img src=".\out\algo1_t_only.png" alt="p1_visual", style="max-height: 200px;">
            </div>

            <p>
                The sampling process follows the diagram.
            </p>
            
            <div class="image-item"></div>
            <img src=".\out\algo2_t_only.png" alt="p1_visual", style="max-height: 200px;">
            </div>

            <p>
                The training loss curve plot, and sampling results for the time-conditioned UNet for 5 and 20 epochs are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5b_2_1_5.png" alt="p1_visual", style="max-height: 300px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_2_1_20.png" alt="p1_visual", style="max-height: 300px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_2_1_loss.png" alt="p1_visual", style="max-height: 400px;">
            </div>


        </p>
    </section>
    <hr>

    <section>
        <h2>2.4 Adding Class-Conditioning to UNet & 2.5 Sampling from the Class-Conditioned UNet</h2>

        <p>

            <p>
                To make the results better and have more control for image generation, the UNet can be conditioned on the
                class of the digit 0-9. This will require adding 2 more FCBlocks to our UNet, for one-hot vector of c, with dropout
                 where 10& of the time the class conditioning vector is dropped by setting it to 0. The training and sampling follow the diagrams below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\algo3_c.png" alt="p1_visual", style="max-height: 200px;">
            </div>
            
            <div class="image-item"></div>
            <img src=".\out\algo4_c.png" alt="p1_visual", style="max-height: 200px;">
            </div>

            <p>
                The training loss curve plot, and sampling results for the time-conditioned UNet for 5 and 20 epochs are shown below:
            </p>

            <div class="image-item"></div>
            <img src=".\out\5b_2_2_5.png" alt="p1_visual", style="max-height: 300px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_2_2_20.png" alt="p1_visual", style="max-height: 300px;">
            </div>

            <div class="image-item"></div>
            <img src=".\out\5b_2_2_loss.png" alt="p1_visual", style="max-height: 400px;">
            </div>
        </p>
    </section>

    

</body>
</html>
